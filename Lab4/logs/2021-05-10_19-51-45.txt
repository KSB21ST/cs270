import matplotlib.pyplot as plt
from datetime import datetime

import numpy as np

def log():
	# read file into string
	with open('rl_template.py', 'r') as inputfile:
		textstr = inputfile.read()
		fn = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + ".txt"
		
		with open("logs/"+fn, 'w') as outputfile:
			outputfile.write(textstr)

log()

N     = 100  # The goal for truly winning
p     = 0.25 # Probability of winning one bet
gamma = 1    # discount factor

states          = [i for i in range(1, N)]   # The states of the game when defined as a Markov Decision Process.
											 # states[x] represents the state of currently having x chips.
v               = [0 for i in range(0, N+1)] # The current state value function. v[x] is the value of state x.
optimal_actions = [0 for i in range(0, N+1)] # List to represent optimal policy. 
											 # optimal_actions[x] should equal the optimal number of 
											 # coins to bet when you currently have x chips (you're in state x).

											 # For both v and optimal_actions, if x == 0 or N, v[x] = optimal_actions[x] = 0.

### Implement value iteration here ###

def value_iteration(iterations):
	global N, p, gamma, v, states
	prob_w = p
	prob_l = 1 - p
	for i in range(iterations):
		for state in states:
			action_space = [i for i in range(1, min(state+1, N-state+1))]
			_values = []
			for action in action_space:
				if(state + action != N):
					r = 0
				else:
					r = 1
				lose_value = prob_l * (gamma * v[state-action])
				win_value = prob_w * (r + gamma * v[state+action])
				_values.append(win_value + lose_value)
			max_value_index = np.argmax(np.asarray(_values))
			v[state] = _values[max_value_index]
	return v

def policy_result():
	global N, p, gamma, v, optimal_actions, states
	prob_w = p
	prob_l = 1 - p
	for state in states:
		action_space = [i for i in range(1, min(state+1,N-state+1))]
		_values = []
		for action in action_space:
			if(state + action != N):
				r = 0
			else:
				r = 1
			win_value = prob_w * (r + gamma * v[state + action])
			lose_value = prob_l * (gamma * v[state - action])
			_values.append(win_value + lose_value)
		max_value_index = np.argmax(np.asarray(_values))
		optimal_actions[state] = max_value_index
	return optimal_actions

v = value_iteration(1000)
optimal_actions = policy_result()



######################################

# Plot state value function for every state
fig = plt.figure()
plt.plot(states, v[1:-1])
plt.show()

# Plot optimal policy for every state
plt.plot(states, optimal_actions[1:-1], 'o')
plt.show()